{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff30304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27ef012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_stopwords():\n",
    "\n",
    "    # Converts the stopwords file into a list and appends 'id', 'text' and 'headline' as stopwords so these aren't\n",
    "    # considered in the inverted index\n",
    "\n",
    "    stopwords_file = open('stopwordsfile.txt', 'r').readlines()\n",
    "    stopwords = []\n",
    "\n",
    "    for word in stopwords_file:\n",
    "        stopwords.append(word.strip())\n",
    "\n",
    "    stopwords.append('id')\n",
    "    stopwords.append('text')\n",
    "    stopwords.append('headline')\n",
    "    \n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb77a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of dictionary objects which are the original files by {doc number: text}\n",
    "# and writes the preprocessed text to a txt file\n",
    "\n",
    "def split_file(file_name):\n",
    "\n",
    "    # splits each document into string lists [doc_id, text, text .. ] for easier preprocessing\n",
    "    # get the list of documents positions [0, 3, 6, 9, .....]\n",
    "    file = open(file_name, 'r').readlines()\n",
    "\n",
    "    file_pos = []\n",
    "    for item in file:\n",
    "        if re.match('^ID:', item):\n",
    "            file_pos.append(file.index(item))\n",
    "    file_pos.append(len(file))\n",
    "\n",
    "    # list of position of every file [[0,3] , [3,6] , ....]\n",
    "    # i -> i+2 corresponds for: [id , headline , text]\n",
    "    positions = ([file_pos[i:i + 2] for i in range(len(file_pos) + 1 - 2)])\n",
    "    newfile = []\n",
    "    for i in positions:\n",
    "        [a, b] = i\n",
    "        newfile.append(file[a:b])\n",
    "\n",
    "    return newfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e83e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(file_name):\n",
    "\n",
    "    print(\"\\nBUILDING INDEX\\n...\")\n",
    "\n",
    "    stopwords = sort_stopwords()\n",
    "    inv_index = []\n",
    "\n",
    "    # Preprocesses and indexes collection per document.============================================================\n",
    "\n",
    "    for document in split_file(file_name):\n",
    "        docnumber = re.sub(\"[^0-9]\", '', document[0])\n",
    "        document.pop(0)     # remove ID line\n",
    "\n",
    "        # Tokenization\n",
    "        text = ', '.join(document)\n",
    "        text.replace('\\n', '')\n",
    "        tokenizedline = re.split('[\\W]', text)\n",
    "        tokenizedline = filter(None, tokenizedline)\n",
    "\n",
    "        # Case folding & Stemming\n",
    "        processed_text = []\n",
    "        for word in tokenizedline:\n",
    "            word = word.lower()\n",
    "            if word not in stopwords and not word.isdigit() :\n",
    "                processed_text.append(stem(word))\n",
    "\n",
    "        # remove headlin and text \n",
    "        if 'headlin' in processed_text:\n",
    "            processed_text.remove('headlin')\n",
    "        if 'text' in processed_text:\n",
    "            processed_text.remove('text')\n",
    "\n",
    "        # Builds an index for each document then appends each to a large index for full collection=================\n",
    "        indexes_per_document = []\n",
    "\n",
    "        for word in processed_text:\n",
    "            word_occurrences = {}\n",
    "            term_obj = {}\n",
    "            positions = [i+1 for i, x in enumerate(processed_text) if x == word] # All positions of a word per document\n",
    "            word_occurrences[docnumber] = positions     # Dictionary for {document:[list of positions in doc]}\n",
    "            term_obj[word] = word_occurrences           # Dictionary for {term: {document: [list of positions in doc]}}\n",
    "            if term_obj not in indexes_per_document:    # avoid repitition\n",
    "                indexes_per_document.append(term_obj)\n",
    "        inv_index.append(indexes_per_document)\n",
    "\n",
    "    # Sort and group inverted index by word=========================================================================\n",
    "    inv_index = list(itertools.chain.from_iterable(inv_index))\n",
    "    inv_index.sort(key=lambda d: sorted(d.keys()))                            # keys = words in inv_index\n",
    "    inv_index = itertools.groupby(inv_index, key=lambda x: sorted(x.keys()))  # keys = words\n",
    "\n",
    "\n",
    "    # Format and save to index file=================================================================================\n",
    "    f = open('index.txt', 'w')\n",
    "\n",
    "    for word, positions in inv_index:\n",
    "        string_word = \"{}:\\n\".format(''.join(word))\n",
    "        f.write(string_word)\n",
    "        list_positions = []\n",
    "        for x in list(positions):            # {term: {document: [list of positions in doc]}}\n",
    "            for key, v in x.items():\n",
    "                list_positions.append(v)\n",
    "        for item in list_positions:          # {document:[list of positions in doc]}\n",
    "            for doc, pos in item.items():\n",
    "                string_position = \"\\t{}: {}\\n\".format(doc, (','.join(map(str, pos))))\n",
    "                f.write(string_position)\n",
    "        f.write('\\n')\n",
    "\n",
    "    print(\"INDEXING COMPLETE\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05b17e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUILDING INDEX\n",
      "...\n",
      "INDEXING COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply indexation on txt file\n",
    "filename = 'collections\\\\trec.sample.txt'\n",
    "build_index(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c2ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8699c663bae32ad9f68a3d22172cf5e8c50905c9eefbb3d4617232e02802a362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
